{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847278c0",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87a9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, matthews_corrcoef\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer,  confusion_matrix, roc_auc_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import joblib \n",
    "\n",
    "import random\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cfffb7",
   "metadata": {},
   "source": [
    "Upload scaled data file and extrated data file obtained using components from the PCA, MFA and t-SNE methods. Both data files are then merged into a single dataframe named all_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d76d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload CSV files\n",
    "scaled_data = pd.read_csv(\"Data/scaled_data.csv\")\n",
    "extracted_data = pd.read_csv(\"Data/df_extracted_features.csv\")\n",
    "\n",
    "# Concatenate all data in a single dataframe:\n",
    "all_data = pd.concat([scaled_data, extracted_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58020efd",
   "metadata": {},
   "source": [
    "Load the various lists of selected variables generated by the different methods and components created through the various feature extraction techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e274d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import generated and selected variables: \n",
    "import sys\n",
    "sys.path.append(\"Data/\") \n",
    "from feature_extraction import pca_features, mfa_features, tsne_features\n",
    "from feature_selection import mrmr_features, rfe_features, ga_features\n",
    "\n",
    "# Define sets of variables to be evaluated\n",
    "all_features = [mrmr_features, rfe_features, ga_features, pca_features, mfa_features, tsne_features]\n",
    "all_names = [\"mRMR_Features\", \"RFE_Features\",\"GA_Features\" ,\"PCA_Features\", \"MFA_Features\", \"tSNE_Features\"]\n",
    "Class = scaled_data.columns[754]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8ce45",
   "metadata": {},
   "source": [
    "Each subset of variables/components obtained in the variable reduction step will be used in different classification models. To evaluate different techniques for the classification of the subjects into the two groups, two functions are defined:    \n",
    "A `classification` function trains and evaluates different sets of variables using a specified classifier. The evaluation process employs the Leave-One-Group-Out cross-validation method. If a parameter grid is provided, it performs an hyperparameter tuning to maximize the Matthews Correlation Coefficient (MCC) score in the validation data.    \n",
    "A range of performance metrics are calculated with the `metrics_calculation` function. Given that each subject is represented by three different observations, a voting process is applied to classify the subject into either the Parkinson's or healthy group. This process aggregates the predictions from all three observations to determine the final classification for each subject. The majority vote from the individual observations will dictate the subject's group classification. Then different metrics are calculated, including Accuracy, F1-score, Area Under the Curve (AUC), MCC, Kappa, Sensitivity, Specificity, Positive Predictive Value (PPV) and Negative Predictive Value (NPV).    \n",
    "Finally, the `classification` function returns two dataframes, one with the metrics obtained during the training process, and the second one with the metrics obtained for the test set (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f2974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculation(labels, predictions, probabilities, grid_search=None):\n",
    "    '''\n",
    "    Calculates evaluation metrics: Accuracy, F1-score, AUC, MCC, Kappa, Sensitivity, Specificity, PPV and NPV.\n",
    "    Optionally includes best parameters if GridSearch is used.\n",
    "    '''\n",
    "    # Voting final predictions\n",
    "    final_pred = []\n",
    "    for i in range(0, len(labels), 3):  # 3 observations per subject\n",
    "        patient_preds = predictions[i:i + 3]\n",
    "        final = mode(patient_preds).mode.item()\n",
    "        final_pred.append(final)\n",
    "\n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(labels[::3], final_pred).ravel()\n",
    "        \n",
    "    # Metrics calculation\n",
    "    accuracy = accuracy_score(labels[::3], final_pred)\n",
    "    f1 = f1_score(labels[::3], final_pred, average='weighted')\n",
    "    kappa = cohen_kappa_score(labels[::3], final_pred)\n",
    "    mcc = matthews_corrcoef(labels[::3], final_pred)\n",
    "    sensitivity = tp/(tp+fn) if (tp+fn) > 0 else 0\n",
    "    specificity = tn/(tn+fp) if (tn+fp) > 0 else 0\n",
    "    ppv = tp/(tp+fp) if (tp+fp) > 0 else 0\n",
    "    npv = tn/(tn+fn) if (tn+fn) > 0 else 0\n",
    "\n",
    "    # Calculate AUC \n",
    "    auc = roc_auc_score(labels[::3], probabilities[::3]) if probabilities is not None else None\n",
    "\n",
    "    # Create results dictionary\n",
    "    result = {\n",
    "        'accuracy': accuracy,\n",
    "        'F1': f1,\n",
    "        'AUC': auc,\n",
    "        'MCC': mcc,\n",
    "        'Kappa': kappa,\n",
    "        'Sensitibity': sensitivity,\n",
    "        'Specificity':specificity,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv              \n",
    "    }\n",
    "    if grid_search is not None:\n",
    "        result['Best_Param'] = grid_search.best_params_\n",
    "    \n",
    "    else:\n",
    "        result['Best_Param'] = \"-\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c582ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(data, features, features_names, classifier, classifier_name, param_grid=None):\n",
    "    '''\n",
    "    Function to train and evaluate different datasets using a specified classifier.\n",
    "    Evaluation uses Leave-One-Group-Out cross-validation method.\n",
    "    If a param_grid is supplied, it performs hyperparameter tuning to maximize the MCC score.\n",
    "    Returns two dataframes:\n",
    "        - train_results_df: Metrics for training sets.\n",
    "        - test_results_df: Metrics for test sets (validation data).\n",
    "    '''\n",
    "    grup = data[\"id\"]\n",
    "    Class = data.iloc[:, 754]\n",
    "    train_results_dict = {}\n",
    "    test_results_dict = {}\n",
    "\n",
    "    # Leave-One-Group-Out cross-validation\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    for num, feature_set in enumerate(features):\n",
    "        sel_features = data[feature_set]\n",
    "\n",
    "        # Predictions, labels and probabilities\n",
    "        train_pred = []\n",
    "        test_pred = []\n",
    "        train_labels=[]\n",
    "        test_labels = []\n",
    "        \n",
    "        train_probab = []\n",
    "        test_probab = []\n",
    "        \n",
    "        best_model = None\n",
    "\n",
    "        # Cross-validation LOGO\n",
    "        for train_idx, test_idx in logo.split(sel_features, Class, groups=grup):\n",
    "            X_train, X_test = sel_features.iloc[train_idx], sel_features.iloc[test_idx]\n",
    "            y_train, y_test = Class.iloc[train_idx], Class.iloc[test_idx]\n",
    "\n",
    "            if param_grid is not None:\n",
    "                # Perform GridSearchCV\n",
    "                mcc_score = make_scorer(matthews_corrcoef)\n",
    "                grid_search = GridSearchCV(classifier, param_grid, scoring=mcc_score, n_jobs=-1)\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                \n",
    "                best_model = grid_search.best_estimator_\n",
    "\n",
    "                # Predictions\n",
    "                train_predictions = best_model.predict(X_train)\n",
    "                test_predictions = best_model.predict(X_test)                \n",
    "                \n",
    "                if \"predict_proba\" in dir(best_model):            \n",
    "                    train_probab.extend(best_model.predict_proba(X_train)[:, 1])\n",
    "                    test_probab.extend(best_model.predict_proba(X_test)[:, 1])\n",
    "                \n",
    "                elif \"decision_function\" in dir(best_model):\n",
    "                    train_probab.extend(best_model.decision_function(X_train))\n",
    "                    test_probab.extend(best_model.decision_function(X_test))\n",
    "                     \n",
    "            else:\n",
    "                # Train classifier without hyperparameter tuning\n",
    "                classifier.fit(X_train, y_train)\n",
    "                train_predictions = classifier.predict(X_train)\n",
    "                test_predictions = classifier.predict(X_test)\n",
    "                \n",
    "                best_model = classifier\n",
    "                \n",
    "                if \"predict_proba\" in dir(classifier):            \n",
    "                    train_probab.extend(classifier.predict_proba(X_train)[:, 1])\n",
    "                    test_probab.extend(classifier.predict_proba(X_test)[:, 1])\n",
    "                \n",
    "                elif \"decision_function\" in dir(classifier):\n",
    "                    train_probab.extend(classifier.decision_function(X_train))\n",
    "                    test_probab.extend(classifier.decision_function(X_test))\n",
    "                    \n",
    "            # Save predictions\n",
    "            train_pred.extend(train_predictions)\n",
    "            test_pred.extend(test_predictions)\n",
    "            train_labels.extend(y_train)\n",
    "            test_labels.extend(y_test)\n",
    "       \n",
    "        # Save trained model\n",
    "        model_path = f\"Models/{classifier_name}_{features_names[num]}.pkl\"\n",
    "        joblib.dump(best_model, model_path)\n",
    "        \n",
    "        # Calculate metrics for train and test\n",
    "        train_metrics = metrics_calculation(train_labels, train_pred, train_probab, grid_search if param_grid else None)\n",
    "        test_metrics = metrics_calculation(test_labels, test_pred, test_probab, grid_search if param_grid else None)\n",
    "        \n",
    "        # Store results\n",
    "        train_results_dict[features_names[num]] = train_metrics\n",
    "        test_results_dict[features_names[num]] = test_metrics\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    train_results_df = pd.DataFrame.from_dict(train_results_dict, orient='index')\n",
    "    train_results_df.index.name = 'Feature_Type'\n",
    "\n",
    "    test_results_df = pd.DataFrame.from_dict(test_results_dict, orient='index')\n",
    "    test_results_df.index.name = 'Feature_Type'\n",
    "\n",
    "    return train_results_df, test_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1e1ed",
   "metadata": {},
   "source": [
    "## K- Nearest Neighbors (k-NN)\n",
    "\n",
    "The k-NN classifier model is used. Performance is evaluated using different number of neighbors. The number of neighbors is optimized during the cross-validation step for each subset of variables / components to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8b9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Parameters grid\n",
    "param_grid_knn = {'n_neighbors': [1,3,5,7,11,15]}\n",
    "\n",
    "# Classification and performance metrics calculation\n",
    "train_knn_results, knn_results = classification(all_data , all_features, all_names, knn, \"KNN\", param_grid_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3029ea36",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The classification performance for each subset of variables/components is evaluated using the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0140f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Classification and performance metrics calculation\n",
    "train_nb_results, nb_results = classification(all_data , all_features, all_names, nb, \"NaiveBayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e392587",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The classification performance for each subset of variables/components is evaluated using the Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44eaa3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "lr = LogisticRegression(max_iter=200) #increase max number of iterations\n",
    "\n",
    "# Classification and performance metrics calculation\n",
    "train_lr_results, lr_results = classification(all_data , all_features, all_names, lr, \"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc7364",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "The SVM classifier model is used with both a linear kernel and an RBF kernel. Performance is evaluated using different values of `C`and `gamma`. These parameters are optimized during the cross-validation step for each subset of variables / components to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147536f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "svm_l = SVC(kernel = \"linear\", probability=True, class_weight=\"balanced\")\n",
    "svm_r = SVC(kernel = \"rbf\", probability=True, class_weight=\"balanced\")\n",
    "\n",
    "# Parameters grid\n",
    "param_grid_svm = {\n",
    "    'C': [0.01, 0.1, 0.5, 1,10,100],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Classification and performance metrics calculation\n",
    "# Linear model\n",
    "train_svm_l_results, svm_l_results = classification(all_data , all_features, all_names, svm_l, \"SVM_Linear\", param_grid_svm)\n",
    "\n",
    "#RBF model\n",
    "train_svm_r_results, svm_r_results = classification(all_data , all_features, all_names, svm_r, \"SVM_RBF\",param_grid_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650aaf07",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "A Random Forest classifier model is used. Performance is evaluated using different numbers of trees (estimators), reducing the maximum depth to 5 or 10, and increasing the minimum samples split to 5 or 10 to reduce overfitting. These parameters are optimized during the cross-validation step for each subset of variables/components to be tested, with the goal of maximizing the MCC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82accbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\",n_jobs=-1, random_state=123) \n",
    "\n",
    "# Parameters grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [20,50,100,200],\n",
    "    'max_depth': [5,10],\n",
    "    'min_samples_split':[5,10] }\n",
    "\n",
    "# Classification and performance metrics calculation\n",
    "train_rf_results, rf_results = classification(all_data , all_features, all_names, rf, \"RandomForest\",param_grid_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95eb91",
   "metadata": {},
   "source": [
    "## Artificial Neural Network\n",
    "\n",
    "For evaluating an artificial neural network, a similar strategy is employed by using each subset of variables/components obtained in the variable reduction step.    \n",
    "In this ocasion, the `classification` function is not used since small code modifications are applied.   \n",
    "First, a neural network is built using the `Sequential` function from `keras`. The architecture includes a dense hidden layer with ReLU activation and a dropout rate of 0.2 to prevent overfitting. The output layer consists of a single neuron with sigmoid activation designed for a binary classification. The model is compiled using the `binary crossentropy` loss function, and the AUC metric for optimization.       \n",
    "For de cross-validation process, Stratified Group K Fold with 10 splits is used to ensure balanced class distributions within each fold. Hyperparameters optimization is applied over the provided parameter grid to maximize the MCC score on the validation data. Parameters to be optimized include the number of neurons in the hidden layer, and the type of optimizer. The maximum number of epochs is set to 100, with early stopping implemented to finish training if there is no improvement in the model's performance for 10 consecutive epochs.   \n",
    "Performance is assessed using the `metrics_calculation` function, which calculates different evaluation materics as previously explained.\n",
    "Finally, two dataframes are generated: one containing the metrics of the training process, and the other one with the metrics of the test set (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a0e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture:\n",
    "def neural_network(neurons):\n",
    "    tf.random.set_seed(123)\n",
    "    nn = Sequential()   \n",
    "    nn.add(Dense(neurons, activation=\"relu\", input_shape=(sel_features.shape[1],)))  \n",
    "    nn.add(Dropout(0.2))\n",
    "    nn.add(Dense(1, activation=\"sigmoid\"))  # Binary output\n",
    "    nn.compile(loss=\"binary_crossentropy\", metrics=[\"AUC\"])\n",
    "    return nn\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  \n",
    "    patience=10,         \n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "# Compile model\n",
    "nn_model = KerasClassifier(model=neural_network, verbose=0, epochs=100, random_state=123,\n",
    "                          callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\"model__neurons\": [8,16,32,64,128], \n",
    "             \"optimizer\": [\"adam\", \"sdg\", \"rmsprop\"]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "grup = all_data[\"id\"]\n",
    "Class = all_data.iloc[:, 754]\n",
    "train_results_dict = {}\n",
    "test_results_dict = {}\n",
    "\n",
    "# Stratified Group K Fold\n",
    "sgkf = StratifiedGroupKFold(n_splits=10)\n",
    "\n",
    "# Evaluate each set of features\n",
    "for num,feature_set in enumerate(all_features):\n",
    "    sel_features = all_data[feature_set]\n",
    "    results = []        \n",
    "\n",
    "    # List of predictions and labels\n",
    "    train_pred = []\n",
    "    test_pred = []\n",
    "    train_labels=[]\n",
    "    test_labels = []\n",
    "\n",
    "    train_probab = []\n",
    "    test_probab = []    \n",
    "        \n",
    "    # Cross-validation SGKF\n",
    "    for train_idx, test_idx in sgkf.split(sel_features, Class, groups=grup):\n",
    "        X_train, X_test = sel_features.iloc[train_idx], sel_features.iloc[test_idx]\n",
    "        y_train, y_test = Class.iloc[train_idx], Class.iloc[test_idx]\n",
    "               \n",
    "        # Search for best params\n",
    "        mcc_score = make_scorer(matthews_corrcoef)\n",
    "        grid_search = GridSearchCV(nn_model, param_grid, scoring= mcc_score,  n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train, validation_data=(X_test, y_test))         \n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Train and test using best params\n",
    "        train_predictions = best_model.predict(X_train)\n",
    "        test_predictions = best_model.predict(X_test)\n",
    "\n",
    "        # Save probabilitites for AUC calcualtion\n",
    "        train_probab.extend(best_model.predict_proba(X_train)[:, 1])\n",
    "        test_probab.extend(best_model.predict_proba(X_test)[:, 1])\n",
    "                \n",
    "        # Save predictions\n",
    "        train_pred.extend(train_predictions)\n",
    "        train_labels.extend(y_train)\n",
    "        test_pred.extend(test_predictions)\n",
    "        test_labels.extend(y_test)\n",
    "                \n",
    "    #Save model\n",
    "    model_path = f\"Models/NeuralNetwork_{all_names[num]}.pkl\"\n",
    "    joblib.dump(best_model, model_path)\n",
    "    \n",
    "    # Calculate metrics for train and test\n",
    "    train_metrics = metrics_calculation(train_labels, train_pred, train_probab, grid_search if param_grid else None)\n",
    "    test_metrics = metrics_calculation(test_labels, test_pred, test_probab, grid_search if param_grid else None)\n",
    "\n",
    "    # Store results\n",
    "    train_results_dict[all_names[num]] = train_metrics\n",
    "    test_results_dict[all_names[num]] = test_metrics\n",
    "\n",
    "# Convert results to DataFrame\n",
    "train_nn_results = pd.DataFrame.from_dict(train_results_dict, orient='index')\n",
    "train_nn_results.index.name = 'Feature_Type'\n",
    "\n",
    "nn_results = pd.DataFrame.from_dict(test_results_dict, orient='index')\n",
    "nn_results.index.name = 'Feature_Type'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72454d7",
   "metadata": {},
   "source": [
    "## Saving results\n",
    "\n",
    "The performance metrics for both the training and test (validation) data from the optimized models of each classifier are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca35fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save train results\n",
    "train_knn_results.to_csv(\"Results/train_knn_results.csv\", sep=\";\")\n",
    "train_nb_results.to_csv(\"Results/train_nb_results.csv\", sep=\";\")\n",
    "train_lr_results.to_csv(\"Results/train_lr_results.csv\", sep=\";\")\n",
    "train_svm_l_results.to_csv(\"Results/train_svm_l_results.csv\", sep=\";\")\n",
    "train_svm_r_results.to_csv(\"Results/train_svm_r_results.csv\", sep=\";\")\n",
    "train_rf_results.to_csv(\"Results/train_rf_results.csv\", sep=\";\")\n",
    "train_nn_results.to_csv(\"Results/train_nn_results.csv\", sep=\";\")\n",
    "\n",
    "#Save results\n",
    "knn_results.to_csv(\"Results/knn_results.csv\", sep=\";\")\n",
    "nb_results.to_csv(\"Results/nb_results.csv\", sep=\";\")\n",
    "lr_results.to_csv(\"Results/lr_results.csv\", sep=\";\")\n",
    "svm_l_results.to_csv(\"Results/svm_l_results.csv\", sep=\";\")\n",
    "svm_r_results.to_csv(\"Results/svm_r_results.csv\", sep=\";\")\n",
    "rf_results.to_csv(\"Results/rf_results.csv\", sep=\";\")\n",
    "nn_results.to_csv(\"Results/nn_results.csv\", sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
